{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal Opportunity Difference (EOD): EOD is a metric used to evaluate fairness in binary classification. It measures the difference between the true positive rate for the privileged group and the true positive rate for the unprivileged group.\n",
    "\n",
    "Demographic Parity Difference (DPD): DPD is another metric used to evaluate fairness in binary classification. It measures the difference between the proportion of positive predictions for the privileged group and the proportion of positive predictions for the unprivileged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairness_indicators.metrics import (\n",
    "    compute_base_rate,\n",
    "    compute_confusion_matrix,\n",
    "    compute_error_rate,\n",
    "    compute_false_negative_rate,\n",
    "    compute_false_positive_rate,\n",
    "    compute_true_negative_rate,\n",
    "    compute_true_positive_rate,\n",
    "    fairness_indicator,\n",
    ")\n",
    "\n",
    "# Load your dataset and split into training and test sets\n",
    "X_train, y_train = ...\n",
    "X_test, y_test = ...\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute the EOD and DPD\n",
    "eod = fairness_indicator(\n",
    "    y_true=y_test,\n",
    "    y_pred=model.predict(X_test),\n",
    "    sensitive_features=X_test[:, 0],  # the sensitive feature(s) in your dataset\n",
    "    base_rate=compute_base_rate(y_true=y_test, sensitive_features=X_test[:, 0]),\n",
    "    function=compute_true_positive_rate,\n",
    ")\n",
    "dpd = fairness_indicator(\n",
    "    y_true=y_test,\n",
    "    y_pred=model.predict(X_test),\n",
    "    sensitive_features=X_test[:, 0],\n",
    "    base_rate=compute_base_rate(y_true=y_test, sensitive_features=X_test[:, 0]),\n",
    "    function=compute_error_rate,\n",
    ")\n",
    "\n",
    "print(f\"EOD: {eod:.3f}\")\n",
    "print(f\"DPD: {dpd:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Accuracy: Adversarial accuracy is a metric that measures the accuracy of a model on adversarial examples, which are inputs that are specifically crafted to fool the model. Adversarial accuracy is often used to evaluate the robustness of a model to adversarial attacks.\n",
    "\n",
    "Margin: Margin is the difference between the probability assigned by the model to the predicted class and the probability assigned to the next most likely class. A large margin indicates that the model is more confident in its predictions, which can be a sign of robustness.\n",
    "\n",
    "Lipschitz Constant: The Lipschitz constant measures the rate at which a function (in this case, the machine learning model) changes with respect to its input. A lower Lipschitz constant indicates that the model is less sensitive to small changes in the input, which can be a sign of robustness.\n",
    "\n",
    "Error Rate Under Distributional Shift: This metric measures the difference in error rate between the training data and a test dataset that is drawn from a different distribution. A low difference in error rate indicates that the model is more robust to distributional shift.\n",
    "\n",
    "To compute these metrics in Python, you may need to use additional libraries such as cleverhans, ART (Adversarial Robustness Toolbox), or foolbox, which provide built-in methods to generate adversarial examples and evaluate their impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cleverhans\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load your dataset and split into training and test sets\n",
    "X_train, y_train = ...\n",
    "X_test, y_test = ...\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create a TensorFlow session and graph\n",
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32, shape=(None, X_test.shape[1]))\n",
    "y = tf.placeholder(tf.float32, shape=(None,))\n",
    "\n",
    "# Convert the logistic regression model to a TensorFlow graph\n",
    "logits = tf.squeeze(model.predict_proba(x)[:, 1:])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "grads = tf.gradients(loss, x)[0]\n",
    "\n",
    "# Create an adversarial example generator using the Fast Gradient Method\n",
    "fgsm = FastGradientMethod(model_fn=lambda x: logits, sess=sess)\n",
    "adv_x = fgsm.generate(x, eps=0.1, clip_min=0., clip_max=1.)\n",
    "\n",
    "# Evaluate the model on clean and adversarial test data\n",
    "clean_acc = model.score(X_test, y_test)\n",
    "adv_acc = model.score(sess.run(adv_x, feed_dict={x: X_test}), y_test)\n",
    "\n",
    "# Compute the margin\n",
    "probs = model.predict_proba(X_test)[:, 1:]\n",
    "margin = np.max(probs, axis=1) - np.sum(probs * (1 - y_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "print(f\"Clean accuracy: {clean_acc:.3f}\")\n",
    "print(f\"Adversarial accuracy: {adv_acc:.3f}\")\n",
    "print(f\"Margin: {np.mean(margin):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:26:40) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e161004f76439249d23adf9602d696d5e3f4040bf06c57722d8dbd596d6486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
